{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ga33Nh9A5fF"
      },
      "outputs": [],
      "source": [
        "# # After running this cell please restart the Runtime \n",
        "\n",
        "\n",
        "# !pip install beautifulsoup4\n",
        "# !pip install numpy\n",
        "# !pip install requests\n",
        "# !pip install spacy\n",
        "# !pip install trafilatura\n",
        "# !pip install \"urllib3\"\n",
        "# !pip install lxml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDaSngNkBK8b"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import numpy as np\n",
        "import requests\n",
        "from requests.models import MissingSchema\n",
        "import spacy\n",
        "import trafilatura\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "import urllib3, socket\n",
        "from urllib3.connection import HTTPConnection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwDcw6TKjq8y"
      },
      "source": [
        "#Getting Content out from the URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNLfEQD4CP1g"
      },
      "outputs": [],
      "source": [
        "def beautifulsoup_extract_text_fallback(response_content):\n",
        "    \n",
        "    '''\n",
        "    This is a fallback function, so that we can always return a value for text content.\n",
        "    Even for when both Trafilatura and BeautifulSoup are unable to extract the text from a \n",
        "    single URL.\n",
        "    '''\n",
        "\n",
        "    # Create the beautifulsoup object:\n",
        "    # soup = BeautifulSoup(response_content, 'html.parser')\n",
        "    soup = BeautifulSoup(response_content, \"html.parser\")\n",
        "    \n",
        "    # Finding the text:\n",
        "    text = soup.find_all(text=True)\n",
        "  \n",
        "    # Remove unwanted tag elements:\n",
        "    cleaned_text = ''\n",
        "    blacklist = [\n",
        "        '[document]',\n",
        "        'noscript',\n",
        "        'header',\n",
        "        'html',\n",
        "        'meta',\n",
        "        'head', \n",
        "        'input',\n",
        "        'script',\n",
        "        'style',]\n",
        "\n",
        "    # Then we will loop over every item in the extract text and make sure that the beautifulsoup4 tag\n",
        "    # is NOT in the blacklist\n",
        "    for item in text:\n",
        "        if item.parent.name not in blacklist:\n",
        "            cleaned_text += '{} '.format(item)\n",
        "\n",
        "    # Remove any tab separation and strip the text:\n",
        "    cleaned_text = cleaned_text.replace('\\t', '')\n",
        "    return cleaned_text.strip()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBpoHUuYj-ix"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_text_from_single_web_page(url):\n",
        "\n",
        "  # HTTPConnection.default_socket_options = ( \n",
        "  #     HTTPConnection.default_socket_options + [\n",
        "  #     (socket.SOL_SOCKET, socket.SO_SNDBUF, 1000000), #1MB in byte\n",
        "  #     (socket.SOL_SOCKET, socket.SO_RCVBUF, 1000000)\n",
        "  # ])\n",
        "  # resp={\"status_code\":\"\"} \n",
        "  try:\n",
        "    resp = requests.get(url, timeout=5,headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'})\n",
        "  except requests.exceptions.ConnectionError:\n",
        "    # resp.status_code = \"Connection refused\" \n",
        "    return np.nan\n",
        "  # We will only extract the text from successful requests:\n",
        "  # print(resp)\n",
        "  if resp.status_code == 200:\n",
        "      # print(resp.content)\n",
        "      return beautifulsoup_extract_text_fallback(resp.content)\n",
        "  else:\n",
        "      # This line will handle for any failures in both the Trafilature and BeautifulSoup4 functions:\n",
        "      return np.nan\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf0B0wQtj40A"
      },
      "source": [
        "#Getting Url ID and URLs from input file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_HeQcxGFSp0"
      },
      "outputs": [],
      "source": [
        "URL_ID=[]\n",
        "URL=[]\n",
        "df = pd.read_excel(\"InPut_1.xlsx\")\n",
        "\n",
        "for i in range (0,len(df)): \n",
        "  print(i)\n",
        "  temp = list(df.iloc[i])\n",
        "  URL_ID.append(str(temp[:][0]))\n",
        "  x=str(temp[1][:])\n",
        "  if (x[:4]!=\"http\"):\n",
        "    x=\"http://\"+x\n",
        "    \n",
        "  URL.append(x)\n",
        "\n",
        "print(URL_ID)\n",
        "\n",
        "\n",
        "print(URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYUxRHEckcMW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji3p3fvUK0hE"
      },
      "source": [
        "#syllable_count and syllable_word_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZQkPau20doD"
      },
      "source": [
        "#word and sentence  counter And Input String\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBKpEPTArsUp"
      },
      "source": [
        "#checking if word is complex or not \n",
        "\n",
        "#syllable_count and syllable_word_count\n",
        "\n",
        "*   syllable_count\n",
        "*   syllable_word_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AybDrPjoL8iQ"
      },
      "outputs": [],
      "source": [
        "def syllable_and_count(word ,syllable_count,syllable_word_count,complex_word):\n",
        "    count = 0\n",
        "    syllable = []\n",
        "    vowels = \"aeiou\"\n",
        "    for i in word:\n",
        "      if i in vowels:\n",
        "        syllable.append(i)\n",
        "        count+=1\n",
        "    \n",
        "    if (word[-2:]==\"ed\") or (word[-2:]=='es'):\n",
        "      count=-1\n",
        "    if count<0:\n",
        "      count = 0\n",
        "    if count>0:\n",
        "      syllable_count+=len(set(syllable))\n",
        "      if len(set(syllable))>=2:\n",
        "        complex_word+=1\n",
        "    syllable_word_count+=1\n",
        "\n",
        "    return (complex_word,syllable_count,syllable_word_count)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CuVNU_FRyHe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f4ktyzJrlbF"
      },
      "source": [
        "#creating List for Stop word\n",
        "\n",
        "\n",
        "# Collecting All Stop Words in as on list\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFo_1KoPSEh0"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords=(stopwords.words('english'))\n",
        "files_stopwords = [\"Stopwords/StopWords_Auditor.txt\", \n",
        "                   \"Stopwords/StopWords_Currencies.txt\",\n",
        "                   \"Stopwords/StopWords_DatesandNumbers.txt\",\n",
        "                   \"Stopwords/StopWords_Generic.txt\",\n",
        "                   \"Stopwords/StopWords_GenericLong.txt\",\n",
        "                   \"Stopwords/StopWords_Geographic.txt\",\n",
        "                   \"Stopwords/StopWords_Names.txt\"]\n",
        "\n",
        "for i in files_stopwords :\n",
        "  w=open(\"/content/{filename}\".format(filename=i),'r').read().strip().lower()\n",
        "  stopwords.extend(w.split())\n",
        "# stopwords=english_stopwords.extend(stopwords)\n",
        "\n",
        "stopwords.extend([\"./td-block-span12\",\n",
        "                            \"/.content\",\n",
        "                            \"./td-related-span4\",\n",
        "                            \"./block\",\n",
        "                            \"./block1\",\n",
        "                            \"/.td-pb-row\",\n",
        "                            \"/.td-container\",  \n",
        "                            \"/.post\"])\n",
        "\n",
        "stopwords=set(stopwords)\n",
        "len(stopwords)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT4yEEYxrg1J"
      },
      "source": [
        "#Negative words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08vuiJaUrVSp"
      },
      "outputs": [],
      "source": [
        "\n",
        "negative_words=open(\"/content/{filename}\".format(filename=\"negative-words.txt\"),'r').read().strip().lower()\n",
        "negative_words=set(negative_words.split())\n",
        "# print(negative_words)\n",
        "# print(len(negative_words))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "596XtXzXriaZ"
      },
      "source": [
        "#Positive words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2dsQw0MrTnb"
      },
      "outputs": [],
      "source": [
        "positive_words=open(\"/content/{filename}\".format(filename=\"positive-words.txt\"),'r').read().strip().lower()\n",
        "positive_words=set(positive_words.split())\n",
        "# print(positive_words)\n",
        "# print(len(positive_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeeYjPVjSaWY"
      },
      "outputs": [],
      "source": [
        "# declearing  arrays to store outputs \n",
        "POSITIVE_SCORE=[]\n",
        "NEGATIVE_SCORE=[]\n",
        "POLARITY_SCORE=[]\n",
        "SUBJECTIVITY_SCORE=[]\n",
        "AVG_SENTENCE_LENGTH=[]\n",
        "PercentageOfComplexWords=[]\n",
        "FogIndex=[]\n",
        "AVG_NUMBER_OF_WORDS_PER_SENTENCE=[]\n",
        "COMPLEX_WORD_COUNT=[]\n",
        "WORD_COUNT=[]\n",
        "PERSONAL_PRONOUNS=[]\n",
        "SYLLABLE_PER_WORD=[]\n",
        "AVG_WORD_LENGTH=[]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ8TPTCAI4PO"
      },
      "outputs": [],
      "source": [
        "url_no=0\n",
        "print(\"iterating through each url in list\")\n",
        "#iterating through each url in list\n",
        "for url in URL:\n",
        "  url_no+=1\n",
        "  print(\"ongoing URL ====\" , url_no)\n",
        "  doc=\"\"\n",
        "  #declearing variuable for input string\n",
        "  input_string=\"\"\n",
        "  # text_content = [extract_text_from_single_web_page(url) for url in URL]\n",
        "  print(\"iterating completed\")\n",
        "  cleaned_text =   extract_text_from_single_web_page(url)\n",
        "   \n",
        "  # for cleaned_text in text_content:\n",
        "\n",
        "  if str(cleaned_text) != 'nan': #cleaning request (text_content) if it has any nan value\n",
        "\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    # 1. Create an NLP document with Spacy:\n",
        "    doc = nlp(cleaned_text)\n",
        "    input_string=str(doc).lower().strip().split()\n",
        "    # print(\"doc=--------->>>>\\n\",doc)\n",
        "\n",
        "    input_string= str(doc)\n",
        "    input_string=input_string.lower().split()\n",
        "    tokens_without_sw = [word for word in input_string if not word in stopwords] \n",
        "\n",
        "\n",
        "    pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
        "    pronouns = pronounRegex.findall(str(doc))\n",
        "\n",
        "    all_words=0\n",
        "    negative_word_count = 0\n",
        "    positive_word_count = 0\n",
        "    complex_word,syllable_count,syllable_word_count =0,0,0\n",
        "    number_of_sentences=len(list(doc.sents))\n",
        "\n",
        "    for i in tokens_without_sw:\n",
        "      all_words+=1 # word counter\n",
        "\n",
        "      \n",
        "      if i in negative_words: # negative word pass\n",
        "        negative_word_count+=1 # negative word counter\n",
        "\n",
        "      if i in positive_words: # positive word pass\n",
        "        positive_word_count+=1 # positive word counter\n",
        "\n",
        "      complex_word,syllable_count,syllable_word_count=syllable_and_count(i,syllable_count,syllable_word_count,complex_word) #calling syllable_and_count function\n",
        "    if all_words==0:\n",
        "      print(\"*********** All Words are zero ************\")\n",
        "\n",
        "      #setting all valuse to NAN if link responce is NAN\n",
        "      POSITIVE_SCORE.append(\"0\")\n",
        "      NEGATIVE_SCORE.append(\"0\")\n",
        "      POLARITY_SCORE.append(\"0\")\n",
        "      SUBJECTIVITY_SCORE.append(\"0\")\n",
        "      AVG_SENTENCE_LENGTH.append(\"0\")\n",
        "      PercentageOfComplexWords.append(\"0\")\n",
        "      FogIndex.append(\"0\")\n",
        "      AVG_NUMBER_OF_WORDS_PER_SENTENCE.append(\"0\")\n",
        "      COMPLEX_WORD_COUNT.append(\"0\")\n",
        "      WORD_COUNT.append(\"0\")\n",
        "      SYLLABLE_PER_WORD.append(\"0\")\n",
        "      PERSONAL_PRONOUNS.append(\"0\")\n",
        "      AVG_WORD_LENGTH.append(\"0\")\n",
        "      continue\n",
        "\n",
        "\n",
        "    polarity_score = (positive_word_count - negative_word_count) / ((positive_word_count + negative_word_count) + 0.000001) # calculating plock\n",
        "    subjectivity_score = (positive_word_count - negative_word_count) / (all_words + 0.000001) # calculating subjectivity_score\n",
        "\n",
        "    Average_Sentence_Length = all_words / number_of_sentences\n",
        "    Percentage_of_Complex_words = complex_word / all_words\n",
        "    Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_words)\n",
        "\n",
        "\n",
        "    #appending calculation to the specific list Respectively \n",
        "    POSITIVE_SCORE.append(positive_word_count)\n",
        "    NEGATIVE_SCORE.append(negative_word_count)\n",
        "    POLARITY_SCORE.append(polarity_score)\n",
        "    SUBJECTIVITY_SCORE.append(subjectivity_score)\n",
        "    AVG_SENTENCE_LENGTH.append(Average_Sentence_Length)\n",
        "    PercentageOfComplexWords.append(Percentage_of_Complex_words)\n",
        "    FogIndex.append(Fog_Index)\n",
        "    AVG_NUMBER_OF_WORDS_PER_SENTENCE.append(len(doc)/Average_Sentence_Length)\n",
        "    COMPLEX_WORD_COUNT.append(complex_word)\n",
        "    WORD_COUNT.append(all_words)\n",
        "    SYLLABLE_PER_WORD.append(syllable_count/syllable_word_count)\n",
        "    PERSONAL_PRONOUNS.append(len(pronouns))\n",
        "    AVG_WORD_LENGTH.append(len(doc)/all_words)\n",
        "  \n",
        "  else:\n",
        "    print(\"*********** NaN found ************\")\n",
        "\n",
        "    #setting all valuse to NAN if link responce is NAN\n",
        "    POSITIVE_SCORE.append(\"nan\")\n",
        "    NEGATIVE_SCORE.append(\"nan\")\n",
        "    POLARITY_SCORE.append(\"nan\")\n",
        "    SUBJECTIVITY_SCORE.append(\"nan\")\n",
        "    AVG_SENTENCE_LENGTH.append(\"nan\")\n",
        "    PercentageOfComplexWords.append(\"nan\")\n",
        "    FogIndex.append(\"nan\")\n",
        "    AVG_NUMBER_OF_WORDS_PER_SENTENCE.append(\"nan\")\n",
        "    COMPLEX_WORD_COUNT.append(\"nan\")\n",
        "    WORD_COUNT.append(\"nan\")\n",
        "    SYLLABLE_PER_WORD.append(\"nan\")\n",
        "    PERSONAL_PRONOUNS.append(\"nan\")\n",
        "    AVG_WORD_LENGTH.append(\"nan\")\n",
        "\n",
        "\n",
        "  print(f\"completed URL == {url_no}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVcUHZs6W6mK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# creating cata frame \n",
        "df = pd.DataFrame(columns = ['URL_ID' ,'URL' ,  'POSITIVE SCORE' , 'NEGATIVE SCORE' ,'POLARITY SCORE' ,'SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH'])\n",
        "\n",
        "# Adding values to dataframe \n",
        "for i in range (0,len(POSITIVE_SCORE)):\n",
        "# for i in range (394,490):\n",
        "\n",
        "  list_1 = [\n",
        "            URL_ID[i],\n",
        "            URL[i],\n",
        "            POSITIVE_SCORE[i],\n",
        "            NEGATIVE_SCORE[i],\n",
        "            POLARITY_SCORE[i],\n",
        "            SUBJECTIVITY_SCORE[i],\n",
        "            AVG_SENTENCE_LENGTH[i],\n",
        "            PercentageOfComplexWords[i],\n",
        "            FogIndex[i],\n",
        "            AVG_NUMBER_OF_WORDS_PER_SENTENCE[i],\n",
        "            COMPLEX_WORD_COUNT[i],\n",
        "            WORD_COUNT[i],\n",
        "            PERSONAL_PRONOUNS[i],\n",
        "            SYLLABLE_PER_WORD[i],\n",
        "            AVG_WORD_LENGTH[i]\n",
        "            ]\n",
        "  df.loc[len(df)] = list_1\n",
        "\n",
        "#setting file name \n",
        "file_name = 'OutPut_1.xlsx'\n",
        "  \n",
        "# saving the excel\n",
        "df.to_excel(file_name,index=False) \n",
        "# display\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4TqU9p5bWYF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
